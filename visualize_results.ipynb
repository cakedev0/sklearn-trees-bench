{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn Trees Benchmark Results Visualization\n",
    "\n",
    "This notebook visualizes the benchmark results from the orchestration runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Load the summary JSON file from the orchestration run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to results directory\n",
    "results_dir = Path(\"results\")\n",
    "\n",
    "# Find the latest summary file\n",
    "summary_files = sorted(results_dir.glob(\"*_summary.json\"))\n",
    "if not summary_files:\n",
    "    raise FileNotFoundError(\"No summary files found in results directory\")\n",
    "\n",
    "latest_summary = summary_files[-1]\n",
    "print(f\"Loading results from: {latest_summary}\")\n",
    "\n",
    "# Load data\n",
    "with open(latest_summary, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\nLoaded {len(df)} benchmark results\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Models tested:\")\n",
    "print(df['model'].value_counts())\n",
    "print(\"\\nBranches tested:\")\n",
    "print(df['branch'].value_counts())\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(df[['n_samples', 'n_features']].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training times by model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training time by model\n",
    "sns.boxplot(data=df, x='model', y='train_time_mean', ax=axes[0])\n",
    "axes[0].set_title('Training Time by Model')\n",
    "axes[0].set_ylabel('Training Time (seconds)')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Prediction time by model\n",
    "sns.boxplot(data=df, x='model', y='predict_time_mean', ax=axes[1])\n",
    "axes[1].set_title('Prediction Time by Model')\n",
    "axes[1].set_ylabel('Prediction Time (seconds)')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance by Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined size column for better visualization\n",
    "df['dataset_size'] = df['n_samples'].astype(str) + 'x' + df['n_features'].astype(str)\n",
    "\n",
    "# Plot training time by dataset size\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for model in df['model'].unique():\n",
    "    model_data = df[df['model'] == model]\n",
    "    axes[0].plot(\n",
    "        model_data['n_samples'],\n",
    "        model_data['train_time_mean'],\n",
    "        'o-',\n",
    "        label=model,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel('Number of Samples')\n",
    "axes[0].set_ylabel('Training Time (seconds)')\n",
    "axes[0].set_title('Training Time vs Dataset Size')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot prediction time by dataset size\n",
    "for model in df['model'].unique():\n",
    "    model_data = df[df['model'] == model]\n",
    "    axes[1].plot(\n",
    "        model_data['n_samples'],\n",
    "        model_data['predict_time_mean'],\n",
    "        'o-',\n",
    "        label=model,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "axes[1].set_xlabel('Number of Samples')\n",
    "axes[1].set_ylabel('Prediction Time (seconds)')\n",
    "axes[1].set_title('Prediction Time vs Dataset Size')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branch Comparison\n",
    "\n",
    "Compare performance across different scikit-learn branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['branch'].nunique() > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training time by branch\n",
    "    sns.boxplot(data=df, x='branch', y='train_time_mean', hue='model', ax=axes[0])\n",
    "    axes[0].set_title('Training Time by Branch')\n",
    "    axes[0].set_ylabel('Training Time (seconds)')\n",
    "    axes[0].set_xlabel('Branch')\n",
    "    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Prediction time by branch\n",
    "    sns.boxplot(data=df, x='branch', y='predict_time_mean', hue='model', ax=axes[1])\n",
    "    axes[1].set_title('Prediction Time by Branch')\n",
    "    axes[1].set_ylabel('Prediction Time (seconds)')\n",
    "    axes[1].set_xlabel('Branch')\n",
    "    axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Only one branch tested, skipping branch comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Impact Analysis\n",
    "\n",
    "Analyze the impact of different model parameters on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters into separate columns\n",
    "params_df = pd.json_normalize(df['model_params'])\n",
    "df_with_params = pd.concat([df, params_df], axis=1)\n",
    "\n",
    "# Show available parameters\n",
    "print(\"Available parameters:\")\n",
    "print(params_df.columns.tolist())\n",
    "\n",
    "# Example: Plot impact of max_depth if available\n",
    "if 'max_depth' in params_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for model in df_with_params['model'].unique():\n",
    "        model_data = df_with_params[\n",
    "            (df_with_params['model'] == model) & \n",
    "            (df_with_params['max_depth'].notna())\n",
    "        ]\n",
    "        if len(model_data) > 0:\n",
    "            ax.plot(\n",
    "                model_data['max_depth'],\n",
    "                model_data['train_time_mean'],\n",
    "                'o-',\n",
    "                label=model,\n",
    "                alpha=0.7\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Max Depth')\n",
    "    ax.set_ylabel('Training Time (seconds)')\n",
    "    ax.set_title('Impact of max_depth on Training Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model and compute statistics\n",
    "summary = df.groupby('model').agg({\n",
    "    'train_time_mean': ['mean', 'std', 'min', 'max'],\n",
    "    'predict_time_mean': ['mean', 'std', 'min', 'max']\n",
    "}).round(4)\n",
    "\n",
    "print(\"Summary Statistics by Model:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export processed results to CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "csv_file = results_dir / f\"{latest_summary.stem}_processed.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Results exported to: {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
